# Sampling Uncertainty Demo

An interactive web-based demonstration of how sampling rates affect aggregation accuracy in data analysis. This tool visualizes the fundamental tradeoff between computational efficiency (lower sample rates) and statistical precision (higher accuracy).

This represents how Honeycomb's Refinery samples traces. Sampling drastically reduces costs by keeping only a representative sample of data, instead of all of it, when the data is repetitive. The aggregations over the sampled data are as useful as over the original data. However, sampling too heavily impacts the accuracy of aggregations.

// Deployed URL on GitHub Pages
[https://jessitron.github.io/sampling-error-tradeoffs/]()

## How to run

```
python -m http.server 8000
```

## Disclaimer

Code & README generated by Claude and Augment Code. Accuracy is what it is

## Overview

This demo helps data engineers, analysts, and researchers understand:

- How sampling introduces uncertainty in different aggregation functions
- The relationship between sample size and confidence intervals
- Why some metrics (like P99) are more sensitive to sampling than others
- The practical implications of sampling decisions in production systems

## Features

### Interactive Controls

- **Volume**: 100 to 10M events (logarithmic scale)
- **Sample Rate**: 1:1 to 1:1000 (keep every Nth event)
- **Distribution Types**: Exponential, Normal, Uniform, Log-Normal, Bimodal
- **Plot Metric Selector**: Switch between COUNT, SUM, AVERAGE, P99 visualizations

### Aggregation Metrics

- **COUNT**: Total number of events (scaled by sample rate)
- **SUM**: Sum of all event values (scaled by sample rate)
- **AVERAGE**: Mean of event values (unscaled, unbiased estimator)
- **P99**: 99th percentile using nearest-rank method

### Visualizations

- **Confidence Intervals**: 95% CI calculated from multiple simulation runs
- **Simulation Plot**: Scatter plot showing individual simulation results with:
  - Red dots: Individual sampling results
  - Green line: True value
  - Blue dashed line: Sample mean
  - Gray band: 95% confidence interval
- **Real-time Updates**: Live recalculation as parameters change

## Technical Implementation

### Architecture

```
Single HTML file with embedded:
├── CSS styling
├── JavaScript simulation engine
└── Canvas-based plotting
```

### Key Components

#### Data Generation

```javascript
// Configurable probability distributions
const distributions = {
  exponential: () => -Math.log(Math.random()) * 100,
  normal: () => boxMullerTransform(),
  uniform: () => Math.random() * 200,
  // ... etc
};
```

#### Sampling Strategy

- **Systematic sampling**: Every Nth event
- **Memory optimization**: Batch processing for large datasets
- **Performance scaling**: Fewer simulation runs for larger volumes

#### Statistical Calculations

- **P99**: Nearest-rank method (`Math.ceil(0.99 * n) - 1`)
- **Confidence Intervals**: Percentile method (2.5th and 97.5th percentiles)
- **Error Metrics**: Percentage deviation from true values

### Performance Considerations

- **Memory limits**: Maximum 10M events to prevent browser crashes
- **Async processing**: Batch generation with yield points
- **Adaptive simulation**: 10-50 runs based on dataset size
- **Debounced updates**: 100ms delay to prevent excessive recalculation

## Use Cases

### Educational

- Teaching sampling theory concepts
- Demonstrating Central Limit Theorem limitations
- Showing why P99 requires larger samples than averages

### Production Planning

- Determining minimum sample rates for acceptable error bounds
- Understanding metric-specific sampling requirements
- Evaluating tradeoffs between system load and data quality

### Research & Analysis

- Comparing sampling strategies across different data distributions
- Quantifying uncertainty in sampled metrics
- Validating sampling assumptions

## Key Insights Demonstrated

1. **Metric Sensitivity**: P99 shows much higher variance than COUNT/SUM/AVERAGE
2. **Distribution Impact**: Log-normal and bimodal distributions increase P99 uncertainty
3. **Sample Size Effects**: Error decreases roughly as 1/√n for most metrics
4. **Bias vs Variance**: AVERAGE is unbiased but has variance; COUNT/SUM are scaled but stable

## File Structure

```
sampling-uncertainty-demo.html
├── HTML structure
├── CSS styling (embedded)
└── JavaScript engine (embedded)
    ├── Distribution generators
    ├── Sampling algorithms
    ├── Statistical calculations
    ├── Visualization engine
    └── UI event handlers
```

## Code Architecture for AI Agents

### Entry Points

- `updateDisplay()`: Main orchestration function
- `runSimulations()`: Core simulation engine
- `drawSimulationPlot()`: Visualization renderer

### Data Flow

```
User Input → updateDisplay() → runSimulations() → calculateAggregations() → drawSimulationPlot()
     ↓              ↓                ↓                    ↓                    ↓
Controls → Volume/Rate → Generate Events → Calc Metrics → Render Results
```

### Key Functions for Modification

#### Adding New Distributions

(is this up-to-date?)

```javascript
// Add to distributions object
distributions.newDist = () => {
  /* generator function */
};

// Add to HTML select options
<option value="newDist">New Distribution</option>;
```

#### Adding New Metrics

```javascript
// Extend calculateAggregations()
return {
  // existing metrics...
  newMetric: calculateNewMetric(values),
};

// Add UI elements and update logic
```

#### Modifying Sampling Strategy

```javascript
// Replace sampleEvents() function
function sampleEvents(events, sampleRate) {
  // Custom sampling logic here
}
```

### Performance Bottlenecks

1. **Event generation**: O(n) memory allocation
2. **Sorting for P99**: O(n log n) per simulation
3. **Canvas rendering**: Fixed cost per frame
4. **Multiple simulations**: Linear in number of runs

## Browser Compatibility

- **Minimum**: ES6 support (async/await, arrow functions)
- **Canvas**: HTML5 Canvas API required
- **Performance**: Recommended 4GB+ RAM for large datasets

## Extending the Demo

### Adding New Visualizations

1. Extend `drawSimulationPlot()` with new chart types
2. Add corresponding UI controls
3. Update `updatePlotMetric()` handler

### Performance Improvements

2. Streaming processing for massive datasets
3. WebGL for high-performance rendering
4. Statistical approximations for real-time updates

## Mathematical Background

### Sampling Error Theory

- **Standard Error**: σ/√n for sample means
- **Confidence Intervals**: Based on sampling distribution
- **Order Statistics**: For percentile calculations

### Distribution Properties

- **Exponential**: λ=1, memoryless property
- **Normal**: μ=100, σ=20, symmetric
- **Log-Normal**: Heavy right tail affects P99
- **Bimodal**: Tests robustness of estimators

This README provides both human-readable context and structured information for AI agents to understand and extend the codebase effectively.
